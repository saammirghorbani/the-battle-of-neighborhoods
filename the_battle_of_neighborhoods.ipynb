{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPteVhAjvSf"
   },
   "source": [
    "# Using Data Science to find the best place to live in Sydney\n",
    "*The Battle of Neighborhoods - IBM Data Science Capstone Project*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mW3wGGQZTZYy"
   },
   "source": [
    "## Introduction\n",
    "Sydney is a fast growing city with 669 suburbs (as of 1 March 2020) http://www.walksydneystreets.net/suburbssydneyall.htm and has been high up on the list for most liveable cities in the world for several years. It is no suprise that many people dream of visiting or starting a life in Sydney. \n",
    "\n",
    "In 2019 I had the privledge of going on exchange to Sydney for one semester, and I can safely say that it is a great place to be! However, it is also one of the most expensive cities in the world, and with so many suburbs it is hard to know which places that one should consider when choosing where to live. This is where the idea for this project started. \n",
    "\n",
    "What if we could use Data Science to recommend which places would be of interest for someone planning on moving to Sydney, or even just choosing a place to stay temporarily?\n",
    "\n",
    "In this project, the goal is to make suggestions based on three main factors*:\n",
    "* Proximity to recommended venues.\n",
    "* Living expenses.\n",
    "* Safety.\n",
    "\n",
    "___\n",
    "\n",
    "\\**DISCLAIMER: I do not encourage anyone to make their decisions based on the findings in this report alone. There are many more factors to consider than are captured in the scope of this project.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RNqyeOsU_Dg"
   },
   "source": [
    "## Data\n",
    "The data for this project will be gathered from various sources using API calls, web scraping and publicly available datasets. \n",
    "\n",
    "### Proximity to recommended venues\n",
    "Foursquare API will be used for gathering information about the most popular venue types in each suburb. This will be done to recommend the most suitable places to live based on the stakeholder's venues of interest.\n",
    "\n",
    "Australian post codes with coordinates https://www.matthewproctor.com/australian_postcodes. Australia Post charges for using their dataset. Instead, a regularly updated, community maintained dataset will be used for this project. Sydney suburb names with coordinates will be extracted from this dataset and then used with the Foursquare API to list venues in each suburb for recommendation.\n",
    "\n",
    "### Living expenses\n",
    "The median house value in each suburb will be web scraped from http://house.speakingsame.com/suburbtop.php?sta=nsw&cat=HomePrice&name=. Geojson data will be gathered from https://data.gov.au/dataset/ds-dga-91e70237-d9d1-4719-a82f-e71b811154c6/details which contains suburb names and boundary coordinates. This will be used for visualizing suburbs with boundaries. The geojson data will then be used together with the median house values to get an overview of which areas are less or more expensive to live in Sydney. \n",
    "\n",
    "### Safety\n",
    "Crime reports in CSV format from 2019 for every Local Government Area (LGA) in Sydney was retrieved from the NSW Bureau of Crime Statistics and Research https://www.bocsar.nsw.gov.au/Pages/bocsar_crime_stats/bocsar_latest_quarterly_and_annual_reports.aspx. This will be done so that a stakeholder can find the safest regions in Sydney. LGA geojson was gathered from https://data.gov.au/dataset/ds-dga-f6a00643-1842-48cd-9c2f-df23a3a1dc1e/details to use for visualizing areas in a choropleth map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dTNbSQhzJkq"
   },
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing necessary libraries and select the coordinates for Sydney that the map visualizations will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXfTMToxqqtg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "#!conda install -c conda-forge folium --yes\n",
    "import folium\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# display all dataframe rows\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "sydney_coords = [-33.8688, 151.2093] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset for venues\n",
    "First we'll need to create a dataset with all Sydney suburbs and their corresponding coordinates. I use a community maintained dataset where I have manually changed a few inaccurate coordinates for some suburbs. This dataset contains all australian postcodes, so I use merge it with another dataset only containing Sydney postcodes and suburb names. I select the intersection of these two datasets by using inner join, in order to get a dataset with Sydney suburbs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "7amf8LRTsxiZ",
    "outputId": "eea7b8f2-fb4b-448c-f3a9-6de24d3c42dc"
   },
   "outputs": [],
   "source": [
    "df_coord = pd.read_csv('https://github.com/saammirghorbani/australianpostcodes/raw/master/australian_postcodes.csv', usecols=['postcode', 'locality', 'long', 'lat'])\n",
    "df_subs = pd.read_csv('sydney_suburbs_postcodes.csv')\n",
    "\n",
    "\n",
    "# Name must be uppercase to match df_cords 'locality' column \n",
    "df_subs['locality'] = df_subs['locality'].str.upper()\n",
    "df_vnu = pd.merge(df_coord, df_subs, how='inner', on=['locality', 'postcode'])\n",
    "# Give better names to columns\n",
    "\n",
    "df_vnu.rename(columns={'locality':'suburb'}, inplace=True)\n",
    "\n",
    "df_vnu.dropna(inplace=True)\n",
    "\n",
    "print(df_vnu.shape)\n",
    "df_vnu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with this dataset is that there are many neighboring suburbs that share the same coordinates. This means that we shouldn't use these coordinates to look for venues, as they would return the same venues for all the neighboring suburbs, and in the end it would incorrectly cluster them in the same group even though they most likely have different compositions of venues. \n",
    "\n",
    "I have manually updated the coordinates on some of these suburbs, but since it would take a very long time to go through all, we will simply keep a subset of suburbs that don't share coordinates for our clustering on venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop suburbs that have the same coordinates\n",
    "df_vnu.drop_duplicates(subset=['long', 'lat'], inplace=True)\n",
    "print(df_vnu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foursquare API\n",
    "In the next stage I want to get the most popular venues for each suburb. For this I make calls to Foursquare API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '5XXK41DFPB522NKMS0JSWEE0EQ0PC0JKPCZ14IYOFI3EWURX'#'CRALOD0T0CQVYXKE1VXXA0BMCD1HEIBU3VGFIFJBVRPP1W0J'#'2LPZZ10LCNP1RNKTFGKGUWRSIS5MXYSFU1F53VVZ3XASYOXO'#'5XXK41DFPB522NKMS0JSWEE0EQ0PC0JKPCZ14IYOFI3EWURX'  # your Foursquare ID\n",
    "CLIENT_SECRET = 'THPV0ULRUMHAAV3EOFXWTFPVGCCBPUTJZNOZQYFNPHBFZLYD'#'X4JPJATKV3LOVTPOW5T3OR33MO0XZFPXN03KC5W2EJ15FHGV'#'0CQ4R2URVJDAATZDYKU3OSWUSFTNFE2N45MMKGMWRRQ4JPMB'#  # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a method that given a suburb name with coordinates returns the top 100 veunes within a radius of 500 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_venues(suburb_names, longitudes, latitudes):\n",
    "    LIMIT = 100\n",
    "    radius = 500 \n",
    "    venues_list = []\n",
    "    for name, long, lat in zip(suburb_names, longitudes, latitudes): \n",
    "        \n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            long, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "\n",
    "         # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            long, \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "        print(name)\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Suburb', \n",
    "                  'Suburb Latitude', \n",
    "                  'Suburb Longitude', \n",
    "                  'Venue Category']\n",
    "    return(nearby_venues)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the top 100 venues for every suburb in our dataset. This will take some time so I have created a CSV file containing the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sydney_venues = get_venues(suburb_names=df_vnu['suburb'], longitudes=df_vnu['long'], latitudes=df_vnu['lat'])\n",
    "#sydney_venues.to_csv('sydney_venues.csv',index=False)\n",
    "sydney_venues = pd.read_csv('sydney_venues.csv')\n",
    "sydney_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for K-Means clustering by encoding categorical variables (veunes) into a series of zeroes and ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_enc = pd.get_dummies(sydney_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add suburb column back to dataframe\n",
    "sydney_enc['Suburb'] = sydney_venues['Suburb'] \n",
    "\n",
    "# move suburb column to the first column\n",
    "sydney_enc = sydney_enc[['Suburb'] + list(sydney_enc.columns[:-1])]\n",
    "\n",
    "sydney_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_grouped = sydney_enc.groupby('Suburb').mean().reset_index()\n",
    "sydney_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's write a function to sort the venues in descending order.\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Suburb']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "suburbs_venues_sorted = pd.DataFrame(columns=columns)\n",
    "suburbs_venues_sorted['Suburb'] = sydney_grouped['Suburb']\n",
    "\n",
    "for ind in np.arange(sydney_grouped.shape[0]):\n",
    "    suburbs_venues_sorted.iloc[ind, 1:] = return_most_common_venues(sydney_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "suburbs_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducing results\n",
    "rnd_state = 1  \n",
    "distortions = []\n",
    "features = sydney_grouped.drop('Suburb', 1)\n",
    "kmeans_models = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    kmeans_models.append(KMeans(n_clusters=k, random_state=rnd_state).fit(features))\n",
    "    distortions.append(kmeans_models[k-1].inertia_)\n",
    "    \n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method, we find in the plot that the most optimal K for clustering is 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the optimal K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusters = 3\n",
    "kmeans2 = kmeans_models[kclusters-1]\n",
    "sydney_grouped2 = sydney_grouped.copy()\n",
    "sydney_grouped2['labels'] = kmeans2.labels_\n",
    "df_final=suburbs_venues_sorted.join(sydney_grouped2.set_index('Suburb'), on=['Suburb'])\n",
    "df_final.drop(df_final.iloc[:, num_top_venues+1:-1], inplace=True, axis=1)\n",
    "df_final = df_final.join(df_vnu.set_index('suburb'), on=['Suburb'])\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House values\n",
    "I created a table over suburbs with their median house values by webscraping. Then I removed dollar signs and commas so that the value is displayed as an integer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house = pd.read_csv('sydney_houses_median_value.csv')\n",
    "# uppercase suburb names to match geojson entries\n",
    "df_house['suburb'] = df_house['suburb'].str.upper()\n",
    "df_house.drop(columns=['rank'], inplace=True)\n",
    "# format values for calculation\n",
    "df_house = df_house.replace(r'\\$','', regex=True) \n",
    "df_house = df_house.replace(r',','', regex=True)\n",
    "# make sure the values are of type int\n",
    "df_house['median_value_aud']=df_house['median_value_aud'].astype('int')\n",
    "df_house.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "Lets take a look at the distribution of house values by plotting them in a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Median house price')\n",
    "ax1.set_ylabel('value')\n",
    "ax1.boxplot(df_house['median_value_aud'])\n",
    "ax1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is an extreme outlier. Houses located in the Point Piper neighborhood are significantly more expensive (around \\\\$24,000,000 AUD) than houses in other neighborhoods, so let's remove it in order to better compare prices in a choropleth map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house.drop(df_house.index[df_house['suburb']=='POINT PIPER'], inplace=True)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Median house price')\n",
    "ax1.set_ylabel('value')\n",
    "ax1.boxplot(df_house['median_value_aud'])\n",
    "ax1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime = pd.read_csv('nsw_lga_crime_trends.csv', usecols=['Local Government Area', 'Offence type','Rate per 100,000 population Jan - Dec 2019'])\n",
    "\n",
    "df_crime.rename(columns={'Local Government Area':'region','Offence type':'type','Rate per 100,000 population Jan - Dec 2019':'crimes_2019'}, inplace=True)\n",
    "\n",
    "df_crime['region'] = df_crime['region'].str.upper()\n",
    "\n",
    "df_crime.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes crime data for all regions in the state of NSW, so I only select the Sydney regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_sydney_regions = ['BAYSIDE', 'BLACKTOWN','BURWOOD', 'CAMDEN', 'CAMPBELLTOWN', 'CANADA BAY', 'CANTERBURY-BANKSTOWN','CUMBERLAND'\n",
    "                  ,'FAIRFIELD','GEORGES RIVER','HORNSBY', 'HUNTERS HILL', 'INNER WEST','KU-RING-GAI','LANE COVE',\n",
    "                  'LIVERPOOL', 'MOSMAN', 'NORTH SYDNEY', 'NORTHERN BEACHES', 'PARRAMATTA', 'PENRITH', 'RANDWICK',\n",
    "                  'RYDE', 'STRATHFIELD', 'SUTHERLAND SHIRE', 'SYDNEY', 'THE HILLS SHIRE', 'WAVERLEY', 'WILLOUGHBY', 'WOOLLAHRA']\n",
    "df_crime['region'] = df_crime[df_crime['region'].isin(greater_sydney_regions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I add up all the crime rates per 100,000 population for each region and divide by 100,000 to get the crime rate as a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all crime rates in a region and group by region\n",
    "df_crime['crimes_2019'].replace(to_replace='nc', value=0, inplace=True)\n",
    "df_crime['crimes_2019'] = df_crime['crimes_2019'].astype(float)\n",
    "# get crime rate\n",
    "df_crime = df_crime.groupby(['region']).sum()/100000\n",
    "df_crime= df_crime.sort_values(by='crimes_2019',ascending=False)\n",
    "df_crime.reset_index(inplace=True)\n",
    "df_crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster composition\n",
    "To categorize the clusters we can look at the most common venues in them. We print the 1st, 2nd and 3rd most common venues in order of frequency found among the suburbs in the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_column_indices = [1,2,3]\n",
    "required_column = [list(df_final.columns.values)[i] for i in required_column_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_series_0 = df_final.loc[df_final['labels'] == 0, df_final.columns[0:6]]\n",
    "print(\"Neighborhoods in cluster:\", df_final_series_0['Suburb'].size)\n",
    "for col in required_column:\n",
    "    print('------------------------------------------')\n",
    "    print(df_final_series_0[col].value_counts(ascending = False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, we can summarize neighborhoods in cluster 0 as having lots of fast food restaurant options and yoga studios (interesting combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_series_1 = df_final.loc[df_final['labels'] == 1, df_final.columns[0:6]]\n",
    "print(\"Neighborhoods in cluster:\", df_final_series_1['Suburb'].size)\n",
    "for col in required_column:\n",
    "    print('------------------------------------------')\n",
    "    print(df_final_series_1[col].value_counts(ascending = False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you love cafés then a neighborhood in cluster 1 seems to be the best choice, with over 80% of them having café as their most common venue. Pubs and pizza places are also popular here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_series_2 = df_final.loc[df_final['labels'] == 2, df_final.columns[0:6]]\n",
    "print(\"Neighborhoods in cluster:\", df_final_series_2['Suburb'].size)\n",
    "for col in required_column:\n",
    "    print('------------------------------------------')\n",
    "    print(df_final_series_2[col].value_counts(ascending = False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhoods in cluster 3 seem to be good choices for those looking for parks, yoga studios or egyptian restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing clusters on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=sydney_coords, zoom_start=10)\n",
    "# set color scheme for the clusters\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, kclusters))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(df_final['lat'],df_final['long'], df_final['Suburb'], df_final['labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choropleth map over house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DqNc5A-e2E5X",
    "outputId": "3f59c691-84e0-4905-df84-4ef1aea90b93"
   },
   "outputs": [],
   "source": [
    "with open('sydney_suburbs_geo.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    \n",
    "threshold_scale = np.linspace(df_house['median_value_aud'].min(),\n",
    "                              df_house['median_value_aud'].max(),\n",
    "                              7, dtype=np.dtype('float64'))\n",
    "threshold_scale = threshold_scale.tolist() \n",
    "# make sure that the last value of the list is greater than the maximum value\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 \n",
    "\n",
    "# create a world map\n",
    "world_map = folium.Map(location=sydney_coords, zoom_start=11, tiles='OpenStreetMap')\n",
    "\n",
    "# generate choropleth map\n",
    "folium.Choropleth(\n",
    "    geo_data=data,\n",
    "    data=df_house,\n",
    "    columns=['suburb','median_value_aud'],\n",
    "    key_on='feature.properties.nsw_loca_2',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Median House value',\n",
    "    threshold_scale=threshold_scale,\n",
    "    reset=True\n",
    ").add_to(world_map)\n",
    "\n",
    "folium.LayerControl().add_to(world_map)\n",
    "# display map\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Point Piper, we see that Watsons Bay and Elizabeth Bay (in Sydney Harbour) are the most expensive neighborhoods with most houses starting at \\\\$7,453,600 AUD\n",
    "\n",
    "It is also apparent that house values in coastal areas, eastern suburbs, and neighborhoods surrounding Chatswood and Gordon are higher than in areas further inland.\n",
    "\n",
    "A suburb which stands out is Duffys Forest (north). Being so far away from the city you'd expect it to have a similar house value as surrounding suburbs. The reason why that's not the case is that this suburb has a small population but several big mansions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choropleth map over crime rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the crime rate dataset from above we load the geojson data for NSW regions and display the crime rates on a choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nsw_lga_geo.json') as json_data:\n",
    "    data2 = json.load(json_data)\n",
    "\n",
    "# create a plain world map\n",
    "world_map2 = folium.Map(location=sydney_coords, zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "# generate choropleth map\n",
    "folium.Choropleth(\n",
    "    geo_data=data2,\n",
    "    data=df_crime,\n",
    "    columns=['region','crimes_2019'],\n",
    "    key_on='feature.properties.nsw_lga__3',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Crime rate in 2019',\n",
    "    reset=True\n",
    ").add_to(world_map2)\n",
    "\n",
    "folium.LayerControl().add_to(world_map2)\n",
    "# display map\n",
    "world_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The city region of Sydney stands out as it has the highest crime rate (over 23%), followed by the outer western and southern regions (Penrith and Campbelltown). The crime rate seems to be lowest in the northern regions and around Sydney harbor (excluding the city region), where it is below 6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Based on the results there are a couple of recommendations to make for someone planning on moving to or staying in Sydney..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "the-battle-of-neighborhoods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
